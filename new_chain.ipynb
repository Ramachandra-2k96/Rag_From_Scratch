{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install necessary packages\n",
    "```bash \n",
    "\n",
    "%pip install --quiet unstructured langchain chromadb \"unstructured[all-docs]\" langchain-text-splitters\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "def load_file_preprocess_parallel(local_paths):\n",
    "\n",
    "    def process_file(local_path):\n",
    "        if os.path.exists(local_path):\n",
    "            loader = PyPDFLoader(local_path)\n",
    "            print(f\"Loading PDF from {local_path}...\")\n",
    "            try:\n",
    "                data = loader.load_and_split()\n",
    "                text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=350)\n",
    "                chunks = text_splitter.split_documents(data)\n",
    "                print(f\"PDF from {local_path} loaded successfully.\")\n",
    "                return chunks\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading PDF from {local_path}: {e}\")\n",
    "                return []\n",
    "        else:\n",
    "            print(f\"File not found: {local_path}\")\n",
    "            return []\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        all_chunks = executor.map(process_file, local_paths)\n",
    "    # Flatten list of lists\n",
    "    return [chunk for sublist in all_chunks for chunk in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os\n",
    "\n",
    "def load_embedding_model(embedding_model = \"nomic-embed-text:latest\",file_path = ['IT445BOOKEDIT.pdf']):\n",
    "    try:\n",
    "        file_path = [\n",
    "            'IT445BOOKEDIT.pdf', '21AI641 MOD 1.pdf', '21AI641 MOD 2.pdf',\n",
    "            '21AI641 MOD 3.pdf', '746127128-BI-module-4-notes-1.pdf', 'module 5 21ai641.pdf'\n",
    "        ]\n",
    "        if not os.path.exists('vector_data'):\n",
    "            chunks = load_file_preprocess_parallel(file_path)\n",
    "            # Create a new vector database if it doesn't exist\n",
    "            vector_db = Chroma.from_documents(\n",
    "                documents=chunks, \n",
    "                embedding=OllamaEmbeddings(model=embedding_model, show_progress=True),\n",
    "                collection_name=\"local-rag\",\n",
    "                persist_directory=\"vector_data\"\n",
    "            )\n",
    "            vector_db.persist()\n",
    "            print(\"New vector database created.\")\n",
    "        else:\n",
    "            embedding = OllamaEmbeddings(model=embedding_model,show_progress=True)\n",
    "            # Load the existing vector database\n",
    "            vector_db = Chroma(\n",
    "                collection_name=\"local-rag\",\n",
    "                persist_directory=\"vector_data\",\n",
    "                embedding_function=embedding,\n",
    "            )\n",
    "            vector_db.min_results=10\n",
    "            vector_db.get()\n",
    "            print(\"Loaded existing Chroma database from disk.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing vector database or embeddings: {e}\")\n",
    "    return vector_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ramachandra/.local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing Chroma database from disk.\n",
      "Retriever initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "vector_db = load_embedding_model()\n",
    "# Initialize the LLM\n",
    "local_model = \"gemma2:2b\"\n",
    "prompt_local_model = \"llama3.1:latest\"\n",
    "\n",
    "llm = ChatOllama(model=local_model)\n",
    "prompt_llm = ChatOllama(model=prompt_local_model,temperature=0)\n",
    "\n",
    "# Multi-query prompt for better retrieval\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. While generating new question the meaning of original question should not change. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Set up the retriever\n",
    "    retriever = MultiQueryRetriever.from_llm(\n",
    "        vector_db.as_retriever(), \n",
    "        prompt_llm,\n",
    "        prompt=QUERY_PROMPT,\n",
    "    )\n",
    "    print(\"Retriever initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing retriever: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 10.50it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 16.55it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  7.08it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 18.60it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text focuses on general dashboards and their structure, not specifically on a company called \"Mapie.\"  \n",
      "\n",
      "Here's what we can infer about descriptive analytics from the text:\n",
      "\n",
      "* **Descriptive Analytics:** This type of analysis focuses on summarizing past data to understand current trends. The document mentions these types of insights are often presented in dashboards, with the information categorized into three layers:\n",
      "    * **Monitoring:**  Visually showing key performance metrics to track overall health (e.g., sales revenue).\n",
      "    * **Analysis:** Summarizing dimensional data for deeper insights into problem causes (e.g., identifying why sales are declining). \n",
      "    * **Management:** Detailed operational data leading to actions like resolving issues or making strategic decisions (e.g., how to adjust inventory levels based on recent sales).\n",
      "\n",
      "**Predictive Analytics Support**:  The text does *not* mention predictive analytics in relation to a company named \"Mapie\". To understand what predictive analytics might do, consider: \n",
      "\n",
      "* **Predictive Analytics:** This type of analysis uses past trends and data to forecast future outcomes or potential scenarios. It might involve:\n",
      "    * **Forecasting Sales:**  Analyzing sales patterns to predict future demand.\n",
      "    * **Customer Churn Prediction:** Modeling customer behavior to anticipate who is likely to leave. \n",
      "    * **Inventory Optimization:** Forecasting demand to determine optimal stock levels and minimize waste. \n",
      "\n",
      "**How We Can Get the Answers You Seek**\n",
      "\n",
      "To get answers about \"Mapie\" specifically, you'll need to look for that company's information or documentation: \n",
      "\n",
      "1. **Company Website:** Check if Mapie has a website where they might provide details on their analytics capabilities. \n",
      "2. **Industry Publications:** Look for articles or press releases about Mapie in technology/business publications.  \n",
      "3. **Online Database (e.g., Crunchbase):** Search for \"Mapie\" on online databases, as these often have company profiles that may include information about analytical tools and technologies they use. \n",
      "\n",
      "\n",
      "Let me know if you have more details about the company or would like help exploring specific questions! \n"
     ]
    }
   ],
   "source": [
    "# RAG prompt template\n",
    "template = \"\"\"The answer should satisfy a 10-mark question typically asked in a university exam. The answer you provide should contain at least 500 words, using simple language, and should not omit key points or important technical terms. \n",
    "\n",
    "Please answer the question based ONLY on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "try:\n",
    "    # Create the RAG chain\n",
    "    chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # Get user input for the question\n",
    "    question = '''What information is provided by the descriptive snalytics employed at Mapie.\n",
    "What type of support is provided by the predictive anlaytics employed at Mapie \n",
    "sensing?'''\n",
    "    result = chain.stream(question)\n",
    "    for chunk in result:\n",
    "        print(chunk,end='',flush=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error during RAG chain execution: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 10.21it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 19.64it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  7.01it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 19.07it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document does not provide specific information about **Mapie's descriptive or predictive analytics capabilities**.  \n",
      "\n",
      "Here's why:\n",
      "\n",
      "* **Focus on Dashboard Design:** The text primarily focuses on describing dashboard design and its components, particularly in the context of business intelligence (BI) systems. \n",
      "* **Lack of Specific Case Studies:** It mentions a \"Application Case\" but doesn't delve into details about any specific company or Mapie's analytics solutions.\n",
      "* **Limited Scope of Text:** This excerpt is only a portion of a larger document dealing with BI and dashboard design; it doesn't provide the context needed for mapping out Mapie's analytics capabilities.\n",
      "\n",
      "**To find this information, you'd likely need to explore these sources:**\n",
      "\n",
      "1. **Mapie's Website/Documentation:** Check if they have a dedicated section on their website or product documentation that details their analytics offerings and case studies.\n",
      "2. **Research Papers & Publications:** Explore academic papers, white papers, or technical reports that specifically focus on Mapie's data analysis solutions or business intelligence platforms. \n",
      "3. **Contact Mapie Directly:** Reach out to their sales team or customer service for more information about their analytics capabilities and specific applications.  \n",
      "\n",
      "\n",
      "Let me know if you have any further questions! \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get user input for the question\n",
    "question = '''What information is provided by the descriptive snalytics employed at Mapie.\n",
    "What type of support is provided by the predictive anlaytics employed at Mapie \n",
    "sensing?'''\n",
    "result = chain.stream(question)\n",
    "for chunk in result:\n",
    "    print(chunk,end='',flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 10.62it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 18.49it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  7.03it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 17.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text you've provided doesn't give specific examples or details about Mapie's analytics practices, so I can only offer general information and insights based on what the excerpt suggests:\n",
      "\n",
      "**Descriptive Analytics at Mapie:**\n",
      "\n",
      "* **Overall Performance Visualization:**  Mapie likely uses descriptive analytics to provide a consolidated view of the organization's performance. This involves presenting data in a clear and concise format, often using dashboards or reports to show key performance indicators (KPIs) across different areas of the business. \n",
      "* **Historical & Predictive Data:** The text mentions \"current and forecasted values\" which suggests that Mapie utilizes both historical data for trends analysis as well as forecasting capabilities to predict future trends.\n",
      "\n",
      "**Predictive Analytics at Mapie:**\n",
      "\n",
      "* **Forecasting Capabilities:**  The text highlights predictive analytics' role in forecasting, implying Mapie is capable of using statistical models and machine learning to predict future outcomes based on historical patterns and factors. \n",
      "* **Targeting Customers & Promotions:** This suggests that predictive analytics at Mapie could be used to target customers with relevant offers or promotions based on their past behavior (buying patterns, location data, etc.).\n",
      "\n",
      "\n",
      "**Important Note:**  The text provides an overview of how Mapie uses analytical techniques in general terms. To get specific information about their use, you'd need to refer to their reports, case studies, or other official documentation. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get user input for the question\n",
    "question = '''What information is provided by the descriptive snalytics employed at Mapie.\n",
    "What type of support is provided by the predictive anlaytics employed at Mapie \n",
    "sensing?'''\n",
    "result = chain.stream(question)\n",
    "for chunk in result:\n",
    "    print(chunk,end='',flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
