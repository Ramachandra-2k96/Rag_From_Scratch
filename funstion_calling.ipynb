{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama Function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from datetime import datetime\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtracts a and b.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "@tool\n",
    "def devide(a: float, b: float) -> float:\n",
    "    \"\"\"Devides a and b.\"\"\"\n",
    "    return a / b\n",
    "\n",
    "@tool\n",
    "def use_Internet(query:str)->str:\n",
    "    \"\"\" Use internet if there is a shortage of information or anything related to upto date information\"\"\"\n",
    "    return query\n",
    "\n",
    "@tool\n",
    "def date(a:str)->str:\n",
    "    \"\"\" Tells todays date and time \"\"\"\n",
    "    return datetime.now()\n",
    "\n",
    "tools = [add, subtract, multiply, devide, use_Internet, date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.render import render_text_description\n",
    "render = render_text_description([add, subtract, multiply, devide, use_Internet, date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([('system',f\"you are an ai assistant ,here is the available tools with description {render}, for the give input retrun the name and input of the tool to use.Return your json object blob with both 'name' and 'arguments'keys.and also make sure that returnd argument is in dictonery formate.do not return anything else\") ,('user',\"{input}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"you are an ai assistant ,here is the available tools with description add(a: int, b: int) -> int - Adds a and b.\\nsubtract(a: int, b: int) -> int - Subtracts a and b.\\nmultiply(a: int, b: int) -> int - Multiplies a and b.\\ndevide(a: float, b: float) -> float - Devides a and b.\\nuse_Internet(a: str) -> str - Use internet if there is a shortage of information or anything related to upto date information\\ndate(a: str) -> str - Tells todays date and time, for the give input retrun the name and input of the tool to use.Return your json object blob with both 'name' and 'arguments'keys.and also make sure that returnd argument is in dictonery formate.do not return anything else\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =ChatOllama(model=\"mistral:instruct\",keep_alive=1,format='json')\n",
    "llm_with_tools = model.bind_tools(tools)\n",
    "def selector(response):\n",
    "    return globals()[response['name']](response['arguments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is 3 * 12 + 11 - 49?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = prompt | model | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | JsonOutputParser() | selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'use_Internet', 'arguments': {'query': 'current stock price of tata steel'}}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is current stock price of tata steel?\"\n",
    "print(chain1.invoke({\"input\":query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'add', 'arguments': {'a': 1, 'b': 2}}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is 1+2\"\n",
    "print(chain1.invoke(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current stock price of tata steel\n"
     ]
    }
   ],
   "source": [
    "query = \"What is current stock price of tata steel?\"\n",
    "print(chain.invoke({\"input\":query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479999999\n"
     ]
    }
   ],
   "source": [
    "query = \"What is sum of 199999999 and 280000000\"\n",
    "print(chain.invoke(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <p style=\"font-size: 1em; margin: 0;\">Written by Ramachandra Udupa | <a href=\"https://www.linkedin.com/in/ramachandra-udupa/\" style=\"color: #4CAF50; text-decoration: none;\">Connect on LinkedIn</a></p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
