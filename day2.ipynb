{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama Tool Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for installing required libarary\n",
    "\n",
    "``` bash\n",
    " !pip install langchain_community langchain_ollama \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Simple Usage of langchain for calling ollama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", adapting to changing market conditions and helping steer companies to success."
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model = \"hermes3\",\n",
    "    keep_alive = -1,\n",
    "    temparature = 0.7,\n",
    "    max_tokens = 2048)\n",
    "template = \"\"\"Provide a concise, one-line explanation on \n",
    "{topic} from the perspective of a {profession}.\"\"\"\n",
    "prompt  = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "stream = chain.stream({\"topic\":\"LLMs\",\"profession\":\"shipping magnate\"})\n",
    "for c in stream:\n",
    "    print(c,flush=True,end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ollama Model to get required json type output\n",
    "#### Note\n",
    "Using words like \"strictly\" in the system prompt can significantly alter the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Jhon', 'age': 35, 'favoriteFood': 'pizza'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser,JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import json\n",
    "json_schema = {\n",
    "    \"title\":\"Person\",\n",
    "    \"description\":\"Identify information about a person\",\n",
    "    \"type\":\"object\",\n",
    "    \"properties\":{\n",
    "        \"name\":{\"title\":\"Name\",\"description\":\"the person's name\",\"type\":\"string\"},\n",
    "        \"age\":{\"title\":\"Age\",\"description\":\"the person's age\",\"type\":\"integer\"},\n",
    "       \"favorite_food\": {\n",
    "        \"title\": \"favorite_food\",\n",
    "        \"description\": \"\"\"A detailed description of the person's most preferred dish, \n",
    "including the type of cuisine, specific ingredients, and any unique preparation methods.\"\"\",\n",
    "        \"type\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"required\":['name','age'],\n",
    "}\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"hermes3\",\n",
    "    temperature=0,\n",
    "    format=\"json\",\n",
    "    max_new_tokens =512\n",
    ")\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "    content=\"Please tell me about a person strictly using the following Json schema:\"\n",
    "    ),\n",
    "    HumanMessage(content=\"{schema}\"),\n",
    "    HumanMessage(\n",
    "    content=\"tell me  about a person named Jhon who is 35 years old and loves pizza.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages=messages)\n",
    "dumps = json.dumps(json_schema,indent=3)\n",
    "chain = prompt | llm | JsonOutputParser()\n",
    "result = chain.invoke({\"schema\":dumps})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='{\"' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='response' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\":' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content=' \"' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='Hello' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='!' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content=' How' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content=' can' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content=' I' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content=' help' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content=' you' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content=' today' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='?\"' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='}' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n",
      "content='\\n' id='run-93221631-62d1-455e-a68c-5a02d64e8034'\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "model = ChatOllama(model=\"phi3.5\", keep_alive=1, format='json')\n",
    "r =model.stream(\"hello\")\n",
    "for c in r:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This module demonstrates the use of Langchain to generate a response based on a given context and question.\n",
    "It uses the Ollama model to process the input and return a structured output in JSON format.\n",
    "\n",
    "\n",
    "``` from langchain_experimental.llms.ollama_functions import OllamaFunctions ```\n",
    "##### this method can also be used but it is DEPRECATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Claudia' height=6.0 hair_color='brunette'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_ollama import ChatOllama\n",
    "# Define the Person schema\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"Name of the person\", required=True)\n",
    "    height: float = Field(description=\"The person's height in floating type\",\n",
    "                          required=True)\n",
    "    hair_color: str = Field(description=\"The person's hair color\", required=False)\n",
    "# Provide the context and question\n",
    "context = \"\"\"\n",
    "Alex is 5 feet tall.\n",
    "Claudia is 1 foot taller than Alex and jumps higher than he does.\n",
    "Claudia is brunette and Alex is blonde.\n",
    "\"\"\"\n",
    "question = \"Who is taller?\"\n",
    "# Define the LLaMA template with explicit instructions to include all required fields\n",
    "# this template can be used for all the LLama family models and Hermes family models\n",
    "# You can find the template for an LLM in the realease note of the model in the \n",
    "# OFFICIAL webpage or ollama hub\n",
    "llama3_template = '''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a smart assistant. Based on the following context and question,\n",
    "provide a detailed JSON response \n",
    "including all required fields according to the schema below.\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "CONTEXT: {context}\n",
    "QUESTION: {question}\n",
    "JSON:\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>'''\n",
    "Prompt = PromptTemplate.from_template(llama3_template)\n",
    "# Initialize the ChatOllama model\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0,\n",
    "    format='json',\n",
    ")\n",
    "# Define structured output using the Person schema\n",
    "structured_llm = llm.with_structured_output(Person)\n",
    "# Create the chain\n",
    "chain = Prompt | structured_llm\n",
    "# Invoke the chain with the question and context\n",
    "response = chain.invoke({\n",
    "    \"question\": question,\n",
    "    \"context\": context\n",
    "})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Bind Tools method\n",
    "#### Note\n",
    "\n",
    "Hermes may not always work correctly with some methods, and other models might also experience similar issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_current_weather with arguments {'location': 'Singapore', 'unit': 'celsius'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "model = OllamaFunctions(\n",
    "        model=\"mistral:instruct\",\n",
    "        format='json',\n",
    "    )\n",
    "model = model.bind_tools(\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\":\"get_current_weather\",\n",
    "            \"description\":\"Get the current weather in a give location\",\n",
    "            \"parameters\":{\n",
    "                \"type\":\"object\",\n",
    "                \"properties\":{\n",
    "                    \"location\":{\n",
    "                        \"type\":'string',\n",
    "                        \"description\":\"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\":{\n",
    "                        'type':'string',\n",
    "                        \"enum\":['celsius','fahrenheit'],\n",
    "                    },\n",
    "                },\n",
    "                \"required\":[\"location\",\"unit\"],\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "response = model.invoke(\" what is the weather in Singapore?\")\n",
    "print(response.tool_calls[0]['name'],\"with arguments\",response.tool_calls[0]['args'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended Method\n",
    "\n",
    "This is the best and easiest method for function calling that I recommend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': '70a16d81-9161-47c2-8b4d-c570e748cee6', 'type': 'tool_call'}]\n",
      "[{'name': 'use_Internet', 'args': {'a': 'special news in India'}, 'id': '069946cd-8b16-454d-ae4f-3569d7ad25e1', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtracts a and b.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "@tool\n",
    "def devide(a: float, b: float) -> float:\n",
    "    \"\"\"Devides a and b.\"\"\"\n",
    "    return a / b\n",
    "\n",
    "def use_Internet(a:str)->str:\n",
    "    \"\"\" Use internet if there is a shortage of information or\n",
    "     anything related to upto date information\"\"\"\n",
    "    return a\n",
    "\n",
    "def date(a:str)->str:\n",
    "    \"\"\" Tells todays date and time \"\"\"\n",
    "    return a\n",
    "\n",
    "tools = [add, subtract, multiply, devide, use_Internet, date]\n",
    "\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "model =ChatOllama(model=\"hermes3\")\n",
    "llm_with_tools = model.bind_tools(tools)\n",
    "\n",
    "query = \"What is 3 * 12\"\n",
    "\n",
    "print(llm_with_tools.invoke(query).tool_calls)\n",
    "query = \"What is todays special news in india?\"\n",
    "\n",
    "print(llm_with_tools.invoke(query).tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <p style=\"font-size: 1em; margin: 0;\">Written by Ramachandra Udupa | <a href=\"https://www.linkedin.com/in/ramachandra-udupa/\" style=\"color: #4CAF50; text-decoration: none;\">Connect on LinkedIn</a></p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
